---
widget: blank
weight: 30
headless: true
design:
  columns: '1'
---

# Latest advances and challenges for the co-execution of HPC \& HPDA workloads on supercomputers

Exascale computing offers the promise of supporting disruptive numerical experiments needed to address the pressing scientific, industrial and societal challenges of the 21st century, such as clean energy, health and climate change.
ExaFlop/s supercomputers will provide the compute capabilities to support simulations of yet unseen precision, generating an unprecedented quantity and quality of data.
Only the latest advances in automated data analytics based on machine-learning or statistical analysis will make it possible to get the most knowledge out of the data generated at this scale.

To reach and go beyond Exascale, it is critical to consider the numerical experiment as a whole, encompassing both the simulation (high-performance computing, HPC) and data-analytics (high-performance data-analytics, HPDA) aspects.
Optimized simulation codes, at the core of the numerical experiments, need to be augmented  with  advanced data analytics in tight coupling patterns so as to overcome the widening performance gap between compute and I/O, and leverage new  deep memory hierarchies.
Once this technical barrier overcome, innovative numerical  patterns become accessible where the outcome of analytics is used to steer the simulation.
These new patterns have the potential to greatly increase the scientific return of investment for numerical experiments at all scales from lab-scale clusters to the largest supercomputers.
